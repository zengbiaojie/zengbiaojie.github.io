<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>文章阅读 on 小 z 的金屋</title>
    <link>https://zz.hanhan.live/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/</link>
    <description>Recent content from 小 z 的金屋</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    
    <managingEditor>1046059314@qq.com (zengbiaojie &amp; zhoujuan)</managingEditor>
    <webMaster>1046059314@qq.com (zengbiaojie &amp; zhoujuan)</webMaster>
    
    <copyright>本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！</copyright>
    
    <lastBuildDate>Mon, 04 Aug 2025 15:03:52 +0800</lastBuildDate>
    
    
    <atom:link href="https://zz.hanhan.live/categories/%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB/index.xml" rel="self" type="application/rss&#43;xml" />
    

    
    

    <item>
      <title>【文章笔记】【模型融合】Merging Multi-Task Models via Weight-Ensembling Mixture of Experts</title>
      <link>https://zz.hanhan.live/post/merging-multi-task-models-via-weight-ensembling-mixture-of-experts/</link>
      <pubDate>Mon, 04 Aug 2025 15:03:52 &#43;0800</pubDate>
      <author>1046059314@qq.com (zengbiaojie &amp; zhoujuan)</author>
      <guid>https://zz.hanhan.live/post/merging-multi-task-models-via-weight-ensembling-mixture-of-experts/</guid>
      <description>
        <![CDATA[<h1>【文章笔记】【模型融合】Merging Multi-Task Models via Weight-Ensembling Mixture of Experts</h1><p>作者：zengbiaojie & zhoujuan（1046059314@qq.com）</p>
        
          <p><a href="https://arxiv.org/abs/2402.00433v2">Merging Multi-Task Models via Weight-Ensembling Mixture of Experts</a></p>
<h2 id="1文章内容总结">
<a class="header-anchor" href="#1%e6%96%87%e7%ab%a0%e5%86%85%e5%ae%b9%e6%80%bb%e7%bb%93"></a>
1.文章内容总结
</h2><h3 id="11-背景和挑战">
<a class="header-anchor" href="#11-%e8%83%8c%e6%99%af%e5%92%8c%e6%8c%91%e6%88%98"></a>
1.1 背景和挑战
</h3><p><strong>背景</strong>：深度学习的快速发展推进了向微调大型预训练模型转变用于下游任务，而非从头开始训练。初始在大规模数据集上训练过后，预训练模型具备了出色的常识，并能熟练识别和处理大型数据模式。这些模型在下游任务上微调后能获取特定任务的知识。在这种情况下，将多个特定任务的模型merge成一个统一的模型成为了知识转移和多任务学习的有效、可扩展的策略。</p>
        
        <hr><p>本文2025-08-04首发于<a href='https://zz.hanhan.live/'>小 z 的金屋</a>，最后修改于2025-08-04</p>]]>
      </description>
      
        <category>文章阅读</category>
      
    </item>
    
    

    <item>
      <title>【文章笔记】【模型融合】Editing Models with Task Arithmetic</title>
      <link>https://zz.hanhan.live/post/taskarithmetic/</link>
      <pubDate>Mon, 04 Aug 2025 14:50:52 &#43;0800</pubDate>
      <author>1046059314@qq.com (zengbiaojie &amp; zhoujuan)</author>
      <guid>https://zz.hanhan.live/post/taskarithmetic/</guid>
      <description>
        <![CDATA[<h1>【文章笔记】【模型融合】Editing Models with Task Arithmetic</h1><p>作者：zengbiaojie & zhoujuan（1046059314@qq.com）</p>
        
          <p><a href="https://arxiv.org/abs/2212.04089">Editing Models with Task Arithmetic</a></p>
<h2 id="1-文章总结及思考">
<a class="header-anchor" href="#1-%e6%96%87%e7%ab%a0%e6%80%bb%e7%bb%93%e5%8f%8a%e6%80%9d%e8%80%83"></a>
1. 文章总结及思考
</h2><h2 id="11-背景">
<a class="header-anchor" href="#11-%e8%83%8c%e6%99%af"></a>
1.1 背景
</h2><p>    <strong>背景和动机</strong>：科研人员经常会想要在预训练之后再去编辑模型，来提高在下游任务上的性能，减少偏差或不符合期待的行为，让模型对齐人类偏好，或用新的信息更新模型。</p>
        
        <hr><p>本文2025-08-04首发于<a href='https://zz.hanhan.live/'>小 z 的金屋</a>，最后修改于2025-08-04</p>]]>
      </description>
      
        <category>文章阅读</category>
      
    </item>
    
    

    <item>
      <title>【文章笔记】【模型融合】AdaMerging</title>
      <link>https://zz.hanhan.live/post/adamergin/</link>
      <pubDate>Mon, 04 Aug 2025 14:32:52 &#43;0800</pubDate>
      <author>1046059314@qq.com (zengbiaojie &amp; zhoujuan)</author>
      <guid>https://zz.hanhan.live/post/adamergin/</guid>
      <description>
        <![CDATA[<h1>【文章笔记】【模型融合】AdaMerging</h1><p>作者：zengbiaojie & zhoujuan（1046059314@qq.com）</p>
        
          <p><a href="https://arxiv.org/abs/2310.02575v2">AdaMerging: Adaptive Model Merging for Multi-Task Learning</a></p>
<h1 id="1文章总结及思考">
<a class="header-anchor" href="#1%e6%96%87%e7%ab%a0%e6%80%bb%e7%bb%93%e5%8f%8a%e6%80%9d%e8%80%83"></a>
1.文章总结及思考
</h1><h2 id="11-背景动机和挑战">
<a class="header-anchor" href="#11-%e8%83%8c%e6%99%af%e5%8a%a8%e6%9c%ba%e5%92%8c%e6%8c%91%e6%88%98"></a>
1.1 背景（动机和挑战）
</h2><p> 当前的merge是让多个微调好的模型merge成一个模型后去执行MTL，但这存在几个问题：</p>
        
        <hr><p>本文2025-08-04首发于<a href='https://zz.hanhan.live/'>小 z 的金屋</a>，最后修改于2025-08-04</p>]]>
      </description>
      
        <category>文章阅读</category>
      
    </item>
    
    

    <item>
      <title>【文章笔记】【模型融合】Model Merging in Pre-training of Large Language Models</title>
      <link>https://zz.hanhan.live/post/modelmerging/</link>
      <pubDate>Mon, 04 Aug 2025 12:40:52 &#43;0800</pubDate>
      <author>1046059314@qq.com (zengbiaojie &amp; zhoujuan)</author>
      <guid>https://zz.hanhan.live/post/modelmerging/</guid>
      <description>
        <![CDATA[<h1>【文章笔记】【模型融合】Model Merging in Pre-training of Large Language Models</h1><p>作者：zengbiaojie & zhoujuan（1046059314@qq.com）</p>
        
          <p><a href="https://arxiv.org/abs/2505.12082v1">Model Merging in Pre-training of Large Language Models</a></p>
<h2 id="摘要">
<a class="header-anchor" href="#%e6%91%98%e8%a6%81"></a>
摘要
</h2><p>模型merge已成为增强大型语言模型的一种有前景的技术，尽管它在大规模预训练中的应用仍然相对未被探索。<strong>本文对预训练过程中的模型merge技术进行了全面的研究</strong>。通过对从数百万到超过1000亿个参数的密集和混合专家（MoE）架构的广泛实验，我们证明，merge以恒定学习率训练的检查点不仅可以显著提高性能，还可以准确预测退火行为。这些改进<strong>不仅提高了模型开发的效率，还显著降低了培训成本</strong>。我们对合并策略和超参数的详细消融研究为潜在机制提供了新的见解，同时揭示了新的应用。通过全面的实验分析，我们为有效的模型合并提供了开源社区实用的预训练指南。</p>
        
        <hr><p>本文2025-08-04首发于<a href='https://zz.hanhan.live/'>小 z 的金屋</a>，最后修改于2025-08-04</p>]]>
      </description>
      
        <category>文章阅读</category>
      
    </item>
    
    

    <item>
      <title>【文章笔记】【语义相似性】Advancing Semantic Textual Similarity Modeling</title>
      <link>https://zz.hanhan.live/post/advancing-semantic-textual-similarity-modeling/</link>
      <pubDate>Sun, 03 Aug 2025 01:32:52 &#43;0800</pubDate>
      <author>1046059314@qq.com (zengbiaojie &amp; zhoujuan)</author>
      <guid>https://zz.hanhan.live/post/advancing-semantic-textual-similarity-modeling/</guid>
      <description>
        <![CDATA[<h1>【文章笔记】【语义相似性】Advancing Semantic Textual Similarity Modeling</h1><p>作者：zengbiaojie & zhoujuan（1046059314@qq.com）</p>
        
          <p><a href="https://arxiv.org/abs/2406.05326">Advancing Semantic Textual Similarity Modeling: A Regression Framework with Translated ReLU and Smooth K2 Loss</a></p>
<h2 id="abstract">
<a class="header-anchor" href="#abstract"></a>
Abstract
</h2><p>BERT和RoBERTa的出现使得 STS（Sentence Textual Similarity）得到显著突破，并且对比学习的应用使得STS得到更好的性能。但对比学习的方法难以利用细粒度的标注信息，以及要求大批量的大小以防止模型崩溃。上述挑战都使得 STS 任务受到细微相似度以及在资源有限时的表现。而Sentence-Bert一定程度的解决了上述问题，但Sentence-Bert将 STS 建模为分类任务，作者任务这忽视了语义相似的进步性。因此本文采用回归框架，并提出两个简单有效的loss函数，最终通过实验验证其有效性。</p>
        
        <hr><p>本文2025-08-03首发于<a href='https://zz.hanhan.live/'>小 z 的金屋</a>，最后修改于2025-08-03</p>]]>
      </description>
      
        <category>文章阅读</category>
      
    </item>
    
  </channel>
</rss>
